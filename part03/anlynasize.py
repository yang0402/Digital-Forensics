import requests
import json
import re
from typing import List, Dict, Tuple
import logging
from datetime import datetime
import asyncio
import aiohttp
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SemanticAnalyzer:
    """åŸºäºDeepSeek APIçš„è¯­ä¹‰åˆ†æå™¨"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # é¢„å®šä¹‰çš„æ•æ„Ÿè¯æ±‡ç±»åˆ«
        self.sensitive_categories = {
            "æ¯’å“äº¤æ˜“": [
                "å†°ç³–", "ç™½é¢", "å¤§éº»", "æ‘‡å¤´ä¸¸", "Kç²‰", "æµ·æ´›å› ",
                "è´§", "æ–™", "ç²‰", "ç‰‡", "ä¸¸", "è‰", "å¶å­",
                "ä¸Šè´§", "å‡ºè´§", "æ‹¿è´§", "èµ°è´§", "æ¸…è´§"
            ],
            "æš—è¯­äº¤æ˜“": [
                "èŒ¶å¶", "å’–å•¡", "ç³–æœ", "é¢ç²‰", "è°ƒæ–™",
                "è§é¢", "äº¤æµ", "åˆ‡ç£‹", "ç ”ç©¶", "å“å°"
            ],
            "åœ°ç‚¹ä»£ç ": [
                "è€åœ°æ–¹", "é‚£ä¸ªåœ°æ–¹", "ç†Ÿæ‚‰çš„åœ°æ–¹", "å®‰å…¨å±‹",
                "1å·", "2å·", "Aç‚¹", "Bç‚¹"
            ],
            "æ—¶é—´æš—ç¤º": [
                "è€æ—¶é—´", "å¹³æ—¶", "é‚£ä¸ªæ—¶å€™", "çº¦å®šæ—¶é—´"
            ]
        }

    def create_analysis_prompt(self, text: str, context: str = "") -> str:
        """åˆ›å»ºè¯­ä¹‰åˆ†ææç¤ºè¯"""
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­ä¹‰åˆ†æä¸“å®¶ï¼Œä¸“é—¨è¯†åˆ«æ–‡æœ¬ä¸­çš„æ•æ„Ÿè¯æ±‡å’Œéšå«å«ä¹‰ã€‚

ä»»åŠ¡ï¼šåˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œè¯†åˆ«å¯èƒ½ä¸è¿æ³•æ´»åŠ¨ç›¸å…³çš„æ•æ„Ÿè¯æ±‡ã€æš—è¯­å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚

åˆ†æé‡ç‚¹ï¼š
1. æ¯’å“äº¤æ˜“ç›¸å…³æš—è¯­å’Œä»£è¯
2. äº¤æ˜“æ—¶é—´ã€åœ°ç‚¹çš„éšæ™¦è¡¨è¾¾
3. æ•°é‡ã€ä»·æ ¼çš„æš—ç¤ºæ€§æè¿°
4. ä¸Šä¸‹æ–‡ä¸­çš„å¼‚å¸¸è¡¨è¾¾æ¨¡å¼
5. å¤šé‡å«ä¹‰çš„è¯æ±‡åœ¨ç‰¹å®šè¯­å¢ƒä¸‹çš„çœŸå®æ„å›¾

å¾…åˆ†ææ–‡æœ¬ï¼š
"{text}"

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
"{context}"

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "risk_level": "é«˜é£é™©/ä¸­é£é™©/ä½é£é™©/æ— é£é™©",
    "confidence_score": 0.0-1.0,
    "sensitive_words": [
        {{
            "word": "è¯†åˆ«çš„æ•æ„Ÿè¯",
            "category": "è¯æ±‡ç±»åˆ«",
            "context": "ä¸Šä¸‹æ–‡ç‰‡æ®µ",
            "interpretation": "å¯èƒ½çš„çœŸå®å«ä¹‰",
            "confidence": 0.0-1.0
        }}
    ],
    "semantic_patterns": [
        {{
            "pattern": "è¯†åˆ«çš„è¯­ä¹‰æ¨¡å¼",
            "description": "æ¨¡å¼æè¿°",
            "risk_indicator": "é£é™©æŒ‡æ ‡"
        }}
    ],
    "overall_assessment": "æ•´ä½“è¯„ä¼°å’Œå»ºè®®"
}}
"""
        return prompt

    async def analyze_text_async(self, text: str, context: str = "") -> Dict:
        """å¼‚æ­¥åˆ†ææ–‡æœ¬è¯­ä¹‰"""
        try:
            prompt = self.create_analysis_prompt(text, context)
            
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­ä¹‰åˆ†æå’Œæ•æ„Ÿè¯è¯†åˆ«ä¸“å®¶ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.1,
                "max_tokens": 2000
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        content = result['choices'][0]['message']['content']
                        
                        # å°è¯•è§£æJSONå“åº”
                        try:
                            # æå–JSONéƒ¨åˆ†
                            json_match = re.search(r'\{.*\}', content, re.DOTALL)
                            if json_match:
                                analysis_result = json.loads(json_match.group())
                                return analysis_result
                            else:
                                return self._create_fallback_result(content)
                        except json.JSONDecodeError:
                            return self._create_fallback_result(content)
                    else:
                        logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status}")
                        return self._create_error_result(f"APIè¯·æ±‚å¤±è´¥: {response.status}")
                        
        except Exception as e:
            logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            return self._create_error_result(str(e))

    def analyze_text(self, text: str, context: str = "") -> Dict:
        """åŒæ­¥åˆ†ææ–‡æœ¬è¯­ä¹‰"""
        try:
            prompt = self.create_analysis_prompt(text, context)
            
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­ä¹‰åˆ†æå’Œæ•æ„Ÿè¯è¯†åˆ«ä¸“å®¶ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.1,
                "max_tokens": 2000
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                
                # å°è¯•è§£æJSONå“åº”
                try:
                    json_match = re.search(r'\{.*\}', content, re.DOTALL)
                    if json_match:
                        analysis_result = json.loads(json_match.group())
                        return analysis_result
                    else:
                        return self._create_fallback_result(content)
                except json.JSONDecodeError:
                    return self._create_fallback_result(content)
            else:
                logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return self._create_error_result(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            return self._create_error_result(str(e))

    def _create_fallback_result(self, content: str) -> Dict:
        """åˆ›å»ºå¤‡ç”¨ç»“æœ"""
        return {
            "risk_level": "æœªçŸ¥",
            "confidence_score": 0.0,
            "sensitive_words": [],
            "semantic_patterns": [],
            "overall_assessment": content,
            "error": "JSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹"
        }

    def _create_error_result(self, error_msg: str) -> Dict:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            "risk_level": "é”™è¯¯",
            "confidence_score": 0.0,
            "sensitive_words": [],
            "semantic_patterns": [],
            "overall_assessment": f"åˆ†æå¤±è´¥: {error_msg}",
            "error": error_msg
        }

    def batch_analyze(self, texts: List[str], contexts: List[str] = None) -> List[Dict]:
        """æ‰¹é‡åˆ†ææ–‡æœ¬"""
        if contexts is None:
            contexts = [""] * len(texts)
        
        results = []
        for i, text in enumerate(texts):
            context = contexts[i] if i < len(contexts) else ""
            result = self.analyze_text(text, context)
            result['text_index'] = i
            result['original_text'] = text
            results.append(result)
            
        return results

    async def batch_analyze_async(self, texts: List[str], contexts: List[str] = None) -> List[Dict]:
        """å¼‚æ­¥æ‰¹é‡åˆ†ææ–‡æœ¬"""
        if contexts is None:
            contexts = [""] * len(texts)
        
        tasks = []
        for i, text in enumerate(texts):
            context = contexts[i] if i < len(contexts) else ""
            task = self.analyze_text_async(text, context)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        # æ·»åŠ ç´¢å¼•å’ŒåŸå§‹æ–‡æœ¬
        for i, result in enumerate(results):
            result['text_index'] = i
            result['original_text'] = texts[i]
            
        return results

    def generate_report(self, analysis_results: List[Dict]) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = []
        report.append("="*60)
        report.append("è¯­ä¹‰åˆ†ææŠ¥å‘Š")
        report.append("="*60)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"åˆ†ææ–‡æœ¬æ•°é‡: {len(analysis_results)}")
        report.append("")
        
        # ç»Ÿè®¡é£é™©ç­‰çº§
        risk_stats = {}
        high_risk_texts = []
        
        for result in analysis_results:
            risk_level = result.get('risk_level', 'æœªçŸ¥')
            risk_stats[risk_level] = risk_stats.get(risk_level, 0) + 1
            
            if risk_level == 'é«˜é£é™©':
                high_risk_texts.append(result)
        
        report.append("é£é™©ç­‰çº§ç»Ÿè®¡:")
        for risk, count in risk_stats.items():
            report.append(f"  {risk}: {count}æ¡")
        report.append("")
        
        # è¯¦ç»†åˆ†æé«˜é£é™©æ–‡æœ¬
        if high_risk_texts:
            report.append("é«˜é£é™©æ–‡æœ¬è¯¦ç»†åˆ†æ:")
            report.append("-" * 40)
            
            for i, result in enumerate(high_risk_texts, 1):
                report.append(f"\n{i}. æ–‡æœ¬å†…å®¹: {result.get('original_text', 'N/A')}")
                report.append(f"   é£é™©ç­‰çº§: {result.get('risk_level', 'N/A')}")
                report.append(f"   ç½®ä¿¡åº¦: {result.get('confidence_score', 0):.2f}")
                
                sensitive_words = result.get('sensitive_words', [])
                if sensitive_words:
                    report.append("   è¯†åˆ«çš„æ•æ„Ÿè¯æ±‡:")
                    for word_info in sensitive_words:
                        report.append(f"     - {word_info.get('word', 'N/A')} "
                                    f"({word_info.get('category', 'N/A')}) "
                                    f"ç½®ä¿¡åº¦: {word_info.get('confidence', 0):.2f}")
                        report.append(f"       è§£é‡Š: {word_info.get('interpretation', 'N/A')}")
                
                patterns = result.get('semantic_patterns', [])
                if patterns:
                    report.append("   è¯­ä¹‰æ¨¡å¼:")
                    for pattern in patterns:
                        report.append(f"     - {pattern.get('pattern', 'N/A')}: "
                                    f"{pattern.get('description', 'N/A')}")
                
                report.append(f"   æ•´ä½“è¯„ä¼°: {result.get('overall_assessment', 'N/A')}")
        
        return "\n".join(report)

    def save_results_to_json(self, analysis_results: List[Dict], filename: str = None) -> str:
        """ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"semantic_analysis_results_{timestamp}.json"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "analysis_results"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        filepath = os.path.join(output_dir, filename)
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®ç»“æ„
        output_data = {
            "analysis_info": {
                "generated_at": datetime.now().isoformat(),
                "total_texts": len(analysis_results),
                "analyzer_version": "1.0",
                "api_model": "deepseek-chat"
            },
            "summary_statistics": self._generate_summary_stats(analysis_results),
            "detailed_results": analysis_results
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
            return None

    def _generate_summary_stats(self, analysis_results: List[Dict]) -> Dict:
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "risk_level_distribution": {},
            "high_risk_count": 0,
            "average_confidence": 0.0,
            "total_sensitive_words": 0,
            "sensitive_word_categories": {},
            "semantic_patterns_count": 0
        }
        
        total_confidence = 0.0
        valid_confidence_count = 0
        
        for result in analysis_results:
            # é£é™©ç­‰çº§ç»Ÿè®¡
            risk_level = result.get('risk_level', 'æœªçŸ¥')
            stats["risk_level_distribution"][risk_level] = \
                stats["risk_level_distribution"].get(risk_level, 0) + 1
            
            if risk_level == 'é«˜é£é™©':
                stats["high_risk_count"] += 1
            
            # ç½®ä¿¡åº¦ç»Ÿè®¡
            confidence = result.get('confidence_score', 0)
            if isinstance(confidence, (int, float)) and confidence > 0:
                total_confidence += confidence
                valid_confidence_count += 1
            
            # æ•æ„Ÿè¯ç»Ÿè®¡
            sensitive_words = result.get('sensitive_words', [])
            stats["total_sensitive_words"] += len(sensitive_words)
            
            for word_info in sensitive_words:
                category = word_info.get('category', 'æœªåˆ†ç±»')
                stats["sensitive_word_categories"][category] = \
                    stats["sensitive_word_categories"].get(category, 0) + 1
            
            # è¯­ä¹‰æ¨¡å¼ç»Ÿè®¡
            patterns = result.get('semantic_patterns', [])
            stats["semantic_patterns_count"] += len(patterns)
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        if valid_confidence_count > 0:
            stats["average_confidence"] = total_confidence / valid_confidence_count
        
        return stats


class CaseAnalysisSystem:
    """æ¡ˆä»¶åˆ†æç³»ç»Ÿ"""
    
    def __init__(self, api_key: str):
        self.analyzer = SemanticAnalyzer(api_key)
        self.case_database = []
    
    def add_evidence_text(self, text: str, source: str, metadata: Dict = None):
        """æ·»åŠ è¯æ®æ–‡æœ¬"""
        evidence = {
            "id": len(self.case_database),
            "text": text,
            "source": source,
            "metadata": metadata or {},
            "timestamp": datetime.now().isoformat(),
            "analysis_result": None
        }
        self.case_database.append(evidence)
        return evidence["id"]
    
    def analyze_evidence(self, evidence_id: int) -> Dict:
        """åˆ†æç‰¹å®šè¯æ®"""
        if evidence_id >= len(self.case_database):
            return {"error": "è¯æ®IDä¸å­˜åœ¨"}
        
        evidence = self.case_database[evidence_id]
        context = f"æ¥æº: {evidence['source']}"
        
        analysis_result = self.analyzer.analyze_text(evidence["text"], context)
        evidence["analysis_result"] = analysis_result
        
        return analysis_result
    
    def analyze_all_evidence(self) -> List[Dict]:
        """åˆ†ææ‰€æœ‰è¯æ®"""
        texts = [evidence["text"] for evidence in self.case_database]
        contexts = [f"æ¥æº: {evidence['source']}" for evidence in self.case_database]
        
        results = self.analyzer.batch_analyze(texts, contexts)
        
        # æ›´æ–°æ•°æ®åº“ä¸­çš„åˆ†æç»“æœ
        for i, result in enumerate(results):
            self.case_database[i]["analysis_result"] = result
        
        return results
    
    def get_high_risk_evidence(self) -> List[Dict]:
        """è·å–é«˜é£é™©è¯æ®"""
        high_risk_evidence = []
        for evidence in self.case_database:
            if evidence.get("analysis_result"):
                risk_level = evidence["analysis_result"].get("risk_level")
                if risk_level == "é«˜é£é™©":
                    high_risk_evidence.append(evidence)
        return high_risk_evidence
    
    def generate_case_report(self) -> str:
        """ç”Ÿæˆæ¡ˆä»¶æŠ¥å‘Š"""
        if not self.case_database:
            return "æ²¡æœ‰è¯æ®æ•°æ®"
        
        # ç¡®ä¿æ‰€æœ‰è¯æ®éƒ½å·²åˆ†æ
        unanalyzed = [e for e in self.case_database if not e.get("analysis_result")]
        if unanalyzed:
            logger.info(f"å‘ç°{len(unanalyzed)}æ¡æœªåˆ†æè¯æ®ï¼Œæ­£åœ¨åˆ†æ...")
            self.analyze_all_evidence()
        
        analysis_results = [e["analysis_result"] for e in self.case_database]
        return self.analyzer.generate_report(analysis_results)
    
    def save_case_to_json(self, filename: str = None) -> str:
        """ä¿å­˜æ¡ˆä»¶åˆ†æç»“æœåˆ°JSONæ–‡ä»¶"""
        if not self.case_database:
            logger.warning("æ²¡æœ‰è¯æ®æ•°æ®å¯ä¿å­˜")
            return None
        
        # ç¡®ä¿æ‰€æœ‰è¯æ®éƒ½å·²åˆ†æ
        unanalyzed = [e for e in self.case_database if not e.get("analysis_result")]
        if unanalyzed:
            logger.info(f"å‘ç°{len(unanalyzed)}æ¡æœªåˆ†æè¯æ®ï¼Œæ­£åœ¨åˆ†æ...")
            self.analyze_all_evidence()
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"case_analysis_{timestamp}.json"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "case_results"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        filepath = os.path.join(output_dir, filename)
        
        # å‡†å¤‡å®Œæ•´çš„æ¡ˆä»¶æ•°æ®
        case_data = {
            "case_info": {
                "created_at": datetime.now().isoformat(),
                "total_evidence": len(self.case_database),
                "high_risk_evidence_count": len(self.get_high_risk_evidence()),
                "analysis_version": "1.0"
            },
            "evidence_database": self.case_database,
            "summary_report": self.generate_case_report(),
            "analysis_summary": self.analyzer._generate_summary_stats(
                [e["analysis_result"] for e in self.case_database if e.get("analysis_result")]
            )
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(case_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ¡ˆä»¶åˆ†æç»“æœå·²ä¿å­˜åˆ°: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¡ˆä»¶JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
            return None


# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åˆå§‹åŒ–ç³»ç»Ÿï¼ˆéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥ï¼‰
    API_KEY = "sk-cca4a6b0541a4aef83489d5501813f30"
    
    try:
        # åˆ›å»ºæ¡ˆä»¶åˆ†æç³»ç»Ÿ
        case_system = CaseAnalysisSystem(API_KEY)
        
        # æ·»åŠ ç¤ºä¾‹è¯æ®æ–‡æœ¬
        sample_texts = [
            "ä»Šå¤©é‚£ä¸ªå†°ç³–çš„è´¨é‡ä¸é”™ï¼Œè€åœ°æ–¹è§é¢äº¤æµä¸€ä¸‹",
            "è´§å·²ç»åˆ°äº†ï¼ŒæŒ‰è€ä»·æ ¼ï¼Œéœ€è¦çš„è¯è”ç³»",
            "å’–å•¡è±†æ–°åˆ°äº†ä¸€æ‰¹ï¼Œå“è´¨å¾ˆå¥½ï¼Œæœ‰éœ€è¦å¯ä»¥å“å°",
            "æ˜å¤©è€æ—¶é—´è€åœ°æ–¹ï¼Œå¸¦ä¸Šé‚£ä¸ªä¸œè¥¿",
            "é¢ç²‰ä»·æ ¼æ¶¨äº†ï¼Œç°åœ¨ä¸€åŒ…è¦500",
            "è¿™æ˜¯æ­£å¸¸çš„å•†åŠ¡å¯¹è¯ï¼Œè®¨è®ºäº§å“é”€å”®ç­–ç•¥"
        ]
        
        sources = [
            "æ‰‹æœºçŸ­ä¿¡", "å¾®ä¿¡èŠå¤©", "QQèŠå¤©", 
            "ç”µè¯å½•éŸ³", "é‚®ä»¶", "å·¥ä½œæ–‡æ¡£"
        ]
        
        # æ·»åŠ è¯æ®åˆ°ç³»ç»Ÿ
        for i, text in enumerate(sample_texts):
            case_system.add_evidence_text(text, sources[i])
        
        print("æ­£åœ¨åˆ†æè¯æ®...")
        
        # åˆ†ææ‰€æœ‰è¯æ®
        results = case_system.analyze_all_evidence()
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŠ¥å‘Š
        report = case_system.generate_case_report()
        print(report)
        
        # ä¿å­˜è¯¦ç»†åˆ†æç»“æœåˆ°JSONæ–‡ä»¶
        json_file = case_system.save_case_to_json()
        if json_file:
            print(f"\nâœ… è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°JSONæ–‡ä»¶: {json_file}")
        
        # ä¹Ÿå¯ä»¥å•ç‹¬ä¿å­˜åˆ†æç»“æœ
        analysis_results = [e["analysis_result"] for e in case_system.case_database]
        simple_json_file = case_system.analyzer.save_results_to_json(analysis_results)
        if simple_json_file:
            print(f"âœ… ç®€åŒ–åˆ†æç»“æœå·²ä¿å­˜åˆ°: {simple_json_file}")
        
        # è·å–é«˜é£é™©è¯æ®
        high_risk = case_system.get_high_risk_evidence()
        print(f"\nğŸš¨ å‘ç° {len(high_risk)} æ¡é«˜é£é™©è¯æ®")
        
        # æ˜¾ç¤ºæ–‡ä»¶ä¿å­˜ä½ç½®
        print("\nğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®:")
        print("  - case_results/     (å®Œæ•´æ¡ˆä»¶åˆ†æ)")
        print("  - analysis_results/ (ç®€åŒ–åˆ†æç»“æœ)")
        
    except Exception as e:
        print(f"ç³»ç»Ÿè¿è¡Œé”™è¯¯: {str(e)}")
        print("è¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®")


if __name__ == "__main__":
    main()